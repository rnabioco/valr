---
title: "Interval statistics"
author: "Jay Hesselberth"
date: '`r format(Sys.Date(), "%B %d %Y")`'
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
    vignette: >
      %\VignetteIndexEntry{valr}
      %\VignetteEngine{knitr::rmarkdown}
      %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "img/interval-stats-",
  fig.height = 3,
  fig.align = "center",
  fig.width = 4
)
```

`valr` can be used to explore relationships between sets of intervals. Here we explore the relationship between transcription start sites and repetitive elements in the human genome. 

```{r load-data, message = FALSE, warning = FALSE}
library(valr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(broom)

# load repeats and genes. Data in the valr package is restricted to chr22; the entire
# files can be downloaded from UCSC.
rpts <- read_bed(valr_example('hg19.rmsk.chr22.bed.gz'), n_fields = 6) 
gens <- read_bed(valr_example('hg19.refGene.chr22.bed.gz'), n_fields = 12)

# load chrom sizes
chrs <- read_genome(valr_example('hg19.chrom.sizes.gz'))

# create 1bp intervalss representing transcription start sites
tss <- mutate(gens,
              .start = ifelse(strand == '+', start, end),
              .end = start + 1) %>%
  select(chrom, start = .start, end = .end, name)

tss
```

First we define a function that takes `x` and `y` intervals and computes distance statistics (using `bed_reldist()` and `bed_absdist()`) for specified groups.

```{r stats}
gen_stats <- function(x, y, genome, grp, type = NA) {
  group_by_(x, .dots = grp) %>%
    do(reldist = bed_reldist(., y, detail = TRUE) %>%
         select(.value = .reldist),
       absdist = bed_absdist(., y, genome) %>%
         select(.value = .absdist_scaled)
       ) %>%
    gather_('stat', 'value', setdiff(names(.), list(grp))) %>%
    mutate(type = type)
} 
```

```{r compute}
obs_stats <- gen_stats(rpts, tss, chrs, 'name', 'obs')

shfs <- bed_shuffle(rpts, chrs, within = TRUE)
shf_stats <- gen_stats(shfs, tss, chrs, 'name', 'shuf')

res <- bind_rows(obs_stats, shf_stats) %>%
  unnest(value) %>% 
  group_by(name, stat, type) %>%
  mutate(.id = row_number()) %>%
  spread(type, .value) %>%
  na.omit()

res
```

Now that the data are formatted, we can use `ks.test()` to determine whether there are significant differences between the observed and shuffled data for each group.

```{r pvalues, warning = FALSE}
pvals <- res %>% do(twosided = broom::tidy(ks.test(.$obs, .$shuf)),
                    less = broom::tidy(ks.test(.$obs, .$shuf, alternative = 'less')),
                    greater = broom::tidy(ks.test(.$obs, .$shuf, alternative = 'greater'))) %>%
  gather(alt, type, -name, -stat) %>%
  unnest(type) %>%
  select(name:p.value) %>%
  arrange(p.value)

ggplot(pvals, aes(p.value)) +
  geom_histogram(binwidth = 0.05) +
  facet_grid(stat ~ alt) + theme_bw()
```

We can also assess false discovery rates (q.values) using `p.adjust()`.

```{r qvalues}
pvals <- group_by(pvals, stat, alt) %>%
  mutate(q.value = p.adjust(p.value)) %>%
  ungroup() %>%
  arrange(q.value)
```

Finally we can visualize these results using `stat_ecdf()`.

```{r ecfs}
res_fold <- res %>%
  gather(type, value, -name, -stat, -.id)

signif <- head(pvals, 25)
res_signif <- signif %>% left_join(res_fold, by = c('name','stat'))

ggplot(res_signif, aes(x = value, color = type)) +
  stat_ecdf() + 
  facet_wrap(name ~ stat) + theme_classic() + scale_x_log10()
```

